# -*- coding: utf-8 -*-
"""
Created on Tue May 29 17:10:45 2018

@author: User
"""
import numpy as np
def sigmoid(x):
    return 1/(1+np.exp(-x))


def propagate(w, b, X, Y,number_samples):    
#          MATRIX DIMESNSIONS :
          
#    w= ROW vector with n weights equivalent to the number of features
#    b=scalar quantity
#    X= N x M dimension matrix where M is the number of training samples
#    Y= ROW vector - 1 x M

    # FORWARD PROPAGATION
    #m=X.shape[0]
    m=number_samples
    A = sigmoid( np.dot(w.T,X+b))
    cost =-1/m*np.sum(((Y*np.log(A),(1-Y)*np.log(1-A))))
    #### BACK PROP ####
    dw=1/m*(np.dot(X,(A-Y).T))
    db = (1 / m) * np.sum(A - Y)
    assert(dw.shape == w.shape)
    assert(db.dtype == float)
    cost = np.squeeze(cost)
    assert(cost.shape == ())
    grads = {"dw": dw,
             "db": db}
    return grads, cost

 ############# THE propagate FUNCTION CALCULATES THE GRADIENT FOR ONE ITERATION #######################

def optimise(w,b,X,Y,learning_rate,number_iters,number_samples):
  
    for i in range(0,number_iters):
        grads, cost= propagate(w=w,b=b,X=X,Y=Y,number_samples=number_samples)
        dw=grads['dw']
        db=grads['db']
        w=w-(learning_rate*dw)
        b=b-(learning_rate*dw)
        if i%10==0:
          print(cost)
          #print(db,dw)
    return w,b


def model(X,Y,learning_rate,number_iters):
    ### weight initialise
    m = X.shape[0]
    w=np.zeros((m,1))
    samples=X.shape[1]
    b=0
    #### optimise the function 
    w,b=optimise(w=w,b=b,X=X,Y=Y,learning_rate=learning_rate,number_iters=number_iters,number_samples=samples)
    return sigmoid( np.dot(w.T,X+b))


X, Y = np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[0,0,1]])


model(X,Y,learning_rate=0.1,number_iters=100)
